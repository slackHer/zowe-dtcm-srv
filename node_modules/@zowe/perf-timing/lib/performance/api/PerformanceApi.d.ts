/*!
 * This program and the accompanying materials are made available under the terms of the
 * Eclipse Public License v2.0 which accompanies this distribution, and is available at
 * https://www.eclipse.org/legal/epl-v20.html
 *
 * SPDX-License-Identifier: EPL-2.0
 *
 * Copyright Contributors to the Zowe Project.
 *
 */
import { IPerformanceApi, IPerformanceApiManager } from "../manager/interfaces";
import { ICollectionObserver, IMetrics, INodeTiming, IPerformanceEntry, ISystemInformation } from "./interfaces";
/**
 * A type that generically defines all collection observer maps that are defined
 * to the Performance API.
 *
 * @param T The type of observer stored in the map. The observer must be an
 *          {@link ICollectionObserver} type with entries adhering to the
 *          {@link IPerformanceEntry} interface.
 *
 * @internal
 */
export declare type CollectionMap<T extends ICollectionObserver<IPerformanceEntry>> = Map<string, T>;
/**
 * The underlying api that provides hooks into
 * {@link https://nodejs.org/api/perf_hooks.html Node's Performance Timing APIs}.
 *
 * Use this class in place of relying on Node's experimental APIs.
 *
 * **NOTE:**
 *
 * This class is not intended to be instantiated directly. It should be accessed
 * through the {@link PerfTiming} variable instantiated by this package. Failure
 * to do so will result in lost metrics in the final file output.
 */
export declare class PerformanceApi implements IPerformanceApi {
    private readonly _manager;
    /**
     * Internal method for getting the errors that can be thrown. It is smart
     * and will only do the require once and cache the results for later calls.
     *
     * @internal
     */
    private static readonly _errors;
    /**
     * Cache of loaded error objects. Only populated on first call to {@link PerformanceApi._errors}
     *
     * @internal
     */
    private static _errorImport;
    /**
     * Aggregate all entries present in a {@link CollectionMap} into an array output.
     *
     * Each item present in the map will correspond to a single entry in the final array output. The
     * strings will now become the name property and each data point will be stored in the final
     * object. The statistical analysis present is generated from the IPerformanceEntry instances
     * present in the entries portion of that element.
     *
     *
     * @param map A {@link CollectionMap} of observers containing entries to process.
     * @param T The type representing the raw data stored in the observer. It is important that this
     *          data extends {@link IPerformanceEntry} for proper data processing.
     *
     * @returns An array of {@link IMetric} data points representing each key/value present in the
     *          CollectionMap.
     *
     * @internal
     */
    private static _aggregateData;
    /**
     * Created for consistent reference to Node Performance Timing API.
     *
     * @see {@link PerformanceApi.watch}
     */
    timerify: <T extends (...args: any[]) => any>(fn: T, name?: string) => T;
    /**
     * Created for consistent reference to Node Performance Timing API.
     *
     * @see {@link PerformanceApi.unwatch}
     */
    untimerify: <T extends (...args: any[]) => any>(fn: T, name?: string) => T;
    /**
     * Internal map of all created function timers. The key represents the name
     * and the value is an {@link IFunctionObserver} instance.
     *
     * @internal
     */
    private _functionObservers;
    /**
     * Internal map of all created measurement timers. The key represents the name
     * and the value is an {@link IMeasurementObserver} instance.
     *
     * @internal
     */
    private _measurementObservers;
    /**
     * This variable holds the import from the Node JS performance hooks
     * library.
     *
     * @internal
     */
    private readonly _perfHooks;
    /**
     * Construct the performance API. The constructor is not intended to be used
     * by any class except for the {@link PerformanceApiManager} of this class.
     *
     * @internal
     *
     * @param _manager The manager of this API class.
     */
    constructor(_manager: IPerformanceApiManager);
    /**
     * Clears marks created using {@link mark}.
     *
     * This method does nothing if the Performance API is not enabled. It also takes advantage
     * of the namespace applied within mark; however, if name is not specified, all marks
     * will be cleared from all package.
     *
     * @param name The name of the mark to clear. If not specified, all marks are
     *             cleared.
     *
     * @see https://nodejs.org/api/perf_hooks.html#perf_hooks_performance_clearmarks_name
     */
    clearMarks(name?: string): void;
    /**
     * Aggregate metrics that have been captured for this instance of the API
     * and return them back in a data format.
     *
     * @returns An object representing metrics that have been prepared to be
     *          written to a file.
     *
     * @throws {@link PerformanceNotCapturedError} when the method is called and performance captures
     *                                             are not enabled.
     */
    getMetrics(): IMetrics;
    /**
     * Get the node timing information provided by the underlying node APIs.
     *
     * @returns The node timing information.
     *
     * @throws {@link PerformanceNotCapturedError} when the method is called and performance captures
     *                                             are not enabled.
     */
    getNodeTiming(): INodeTiming;
    /**
     * Gathers and returns valuable system information to help understand the environment
     * where the data was captured.
     *
     * @returns System information formatted in an object ready to write to a file.
     *
     * @throws {@link PerformanceNotCapturedError} when the method is called and performance captures
     *                                             are not enabled.
     */
    getSysInfo(): ISystemInformation;
    /**
     * Creates a mark entry in the Performance Timeline. The time between 2 marks
     * created with this function can be measured in the {@link measure} function.
     *
     * If performance monitoring is not enabled, this function will do nothing.
     *
     * @param name The name of the mark to create. This name will be prefixed with
     *             the namespace calculated by {@link _addPackageNamespace} to
     *             avoid conflicts in the data. This doesn't change how the mark
     *             is referenced. When referencing the mark in the {@link measure}
     *             function, you must use the name passed into this function.
     *
     * @see https://nodejs.org/api/perf_hooks.html#perf_hooks_performance_mark_name
     */
    mark(name: string): void;
    /**
     * The measure method is used to create a new measurement between 2 marks.
     *
     * The measurement name does not have to be unique. Using the same name will add another entry
     * to the corresponding name present in the {@link IMetrics.measurements} array in the
     * formatted run output.
     *
     * Also, just like {@link mark} and {@link clearMarks} the names passed into this function are
     * prefixed with a package namespace. This is due to the possibility of multiple instances of
     * the performance api being present because of being part of multiple dependencies of a project.
     * The namespace will attempt to reduce the clashes between packages.
     *
     * **For example:**
     * Assume that the measure method was called like so:
     *
     * ```TypeScript
     * // marks were created above
     * // api instantiated as api
     * api.measure("name1", "start1", "end1");
     * api.measure("name2", "start2", "end2");
     * api.measure("name2", "start3", "end3");
     * api.measure("name1", "start4", "end4");
     * ```
     *
     * The final result of {@link getMetrics} would look something like this:
     *
     * ```JSON
     * {
     *   "measurements": [
     *     {
     *       "name": "name1",
     *       "entries": [
     *         {"startMark": "start1", "endMark": "end1"},
     *         {"startMark": "start4", "endMark": "end4"}
     *       ]
     *     },
     *     {
     *       "name": "name2",
     *       "entries": [
     *         {"startMark": "start2", "endMark": "end2"},
     *         {"startMark": "start3", "endMark": "end3"}
     *       ]
     *     }
     *   ]
     * }
     * ```
     *
     * @example
     * import { PerfTiming } from "@zowe/perf-timing";
     *
     * // Assume package is example@1.0.0
     *
     * // Tracked internally by node as "example@1.0.0: before loop"
     * PerfTiming.api.mark("before loop");
     *
     * for (let i = 0; i < 10000; i++) {}
     *
     * // Tracked internally by node as "example@1.0.0: after loop"
     * PerfTiming.api.mark("after loop"); // This will be tracked by node as "example@1.0.0: after loop"
     *
     * // References the internally tracked "before loop" and "after loop" marks and creates a measurement
     * // tracked internally as "example@1.0.0: loop".
     * PerfTiming.api.measure("loop", "before loop", "after loop");
     *
     * @param name The name of the measurement. Use this to track multiple measurements under
     *             the same final array output.
     * @param startMark The name of the start mark created with {@link mark}
     * @param endMark The name of the end mark created with {@link mark}
     */
    measure(name: string, startMark: string, endMark: string): void;
    /**
     * Unwatch a function that was {@link watch}ed. It performance is not enabled, it will return
     * the function passed as parameter 1.
     *
     * It is recommended to first check manually if performance is enabled before using the watch
     * and unwatch methods for performance reasons.
     *
     * @example
     * import { PerfTiming } from "@zowe/perf-timing";
     *
     * let fn = () => "test";
     *
     * fn = PerfTiming.api.watch(fn);
     *
     * fn();
     *
     * fn = PerfTiming.api.unwatch(fn);
     *
     * @example
     * // Recommended to check if performance is enabled first due to the need
     * // to assign values.
     * import { PerfTiming } from "@zowe/perf-timing";
     *
     * let fn = () => "test";
     *
     * if (PerfTiming.enabled) // Check if performance is enabled before assigning
     *   fn = PerfTiming.api.watch(fn);
     *
     * fn();
     *
     * if (PerfTiming.enabled) // Check if performance is enabled before assigning
     *   fn = PerfTiming.api.unwatch(fn);
     *
     * @param fn The function to unwatch.
     * @param name The name that the observer was given, only relevant if the observer was given a name
     *             on the {@link watch}. If omitted, the value of fn.name will be used as the name.
     *
     * @param T The type of the function passed in the first parameter.
     *
     * @returns The original implementation of the function that was wrapped in watch or the value
     *          of fn if performance is not enabled.
     *
     * @throws {@link TimerDoesNotExistError} when performance is enabled and there isn't an observer
     *         with the name passed into the function.
     */
    unwatch<T extends (...args: any[]) => any>(fn: T, name?: string): T;
    /**
     * Wraps a function in a node monitoring function. This allows metrics to be captured about the
     * watched function.
     *
     * Watching a function allows the collection of the following items:
     *
     * * The total number of calls to the function while watched.
     * * The time each individual call took and the parameters sent in.
     * * The total time spent in the function across all calls.
     * * The average time per function call.
     *
     * **NOTE:**
     *
     * For proper time tracking to occur, the watch function will return the original function wrapped
     * in a Node tracking function. It is important that the original implementation of the passed
     * function be set equal to the return value of this method for metrics to be gathered. Unlike
     * most languages, there is no way for a parameter to be truly passed by reference in JavaScript.
     * Had this been the case, this function would have directly modified the reference in the first
     * parameter so no extra work would have to be done by the programmer.
     *
     * **NOTE:**
     *
     * Since Node only uses the function name as the measurement name, it is recommended to ensure
     * that the fn.name passed into this function is unique compared to all functions that you are
     * watching (including those from other packages). Failure to do so will result in a measurement
     * being applied to multiple watches.
     *
     * **For Example:** In the code example below, a call to either of the functions will result
     * in both watches capturing the call.
     *
     * ```TypeScript
     * import { PerfTiming } from "@zowe/perf-timing";
     *
     * let fn1 = function test() {
     *   console.log("fn1");
     * }
     *
     * let fn2 = function test() {
     *   console.log("fn2");
     * }
     *
     * fn1 = PerfTiming.api.watch(fn1, "name 1");
     * fn2 = PerfTiming.api.watch(fn2, "name 2");
     *
     * // The below line will log fn1 but will trigger a metric entry in the watch
     * // for both fn1 and fn2 because fn1.name === fn2.name. To counteract this,
     * // simply choose a different name between the two functions.
     * fn1();
     * ```
     *
     * **NOTE:**
     *
     * It is recommended to first check manually if performance is enabled before using the watch
     * and unwatch methods for performance reasons.
     *
     * @example
     * // Track time spent in require calls
     * import { PerfTiming } from "@zowe/perf-timing";
     *
     * if (PerfTiming.enabled) {
     *   const Module = require("module");
     *   // Store the reference to the original require.
     *   const originalRequire = Module.prototype.require;
     *
     *   // Watch a wrapper named function so we can be sure that not just
     *   // any anonymous function gets checked.
     *   Module.prototype.require = PerfTiming.api.timerify(function NodeModuleLoader() {
     *     return originalRequire.apply(this, arguments);
     *   });
     * }
     *
     * // Continue your application code. Also note, there is no need to unwatch a function.
     * // If a function is not unwatched, this will be handled internally by the data collection
     * // functions.
     *
     * @param fn The function to watch. This function will be sent into the
     *           {@link https://nodejs.org/api/perf_hooks.html#perf_hooks_performance_timerify_fn Node Timerify}
     *           function. This will wrap the function passed in a monitoring function.
     * @param name An optional name to give the timer. If the name is not specified, then the value
     *             of fn.name will be used for tracking.
     *
     * @param T The type of the function passed in the first parameter.
     *
     * @returns The watched function (that must replace the original implementation) or the value of
     *          fn if performance is not enabled.
     *
     * @throws {@link TimerNameConflictError} when there is already an active timer of the evaluated
     *         name.
     */
    watch<T extends (...args: any[]) => any>(fn: T, name?: string): T;
    /**
     * Gets a namespace scoped string from an input string.
     *
     * @param value The string to prepend the namespace to.
     * @returns The value with the namespace.
     */
    private _addPackageNamespace;
}
